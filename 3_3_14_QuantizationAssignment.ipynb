{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marciodbotas/MachineLearning/blob/main/3_3_14_QuantizationAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzi8z7HQ0bst"
      },
      "source": [
        "# Quantization in TFLite\n",
        "In this assignment you will get to explore quantizing a model in TFLite again. This time some of the code will be missing and you'll need to fill it in. The model we are exploring today is a computer vision model that recognizes hand gestures for the rock, paper, scissors game!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDxgr2G0yYs"
      },
      "source": [
        "## Set up the problem\n",
        "\n",
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "D1J15Vh_1Jih"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Desired versions\n",
        "tf_version = \"2.14.0\"\n",
        "hub_version = \"0.15.0\"\n",
        "datasets_version = \"4.6.0\"\n",
        "numpy_version = \"1.26.4\"\n",
        "protobuf_version = \"3.20.3\"\n",
        "\n",
        "# Try importing optional packages\n",
        "try:\n",
        "    import tensorflow_hub as hub\n",
        "except ImportError:\n",
        "    hub = None\n",
        "\n",
        "try:\n",
        "    import tensorflow_datasets as tfds\n",
        "except ImportError:\n",
        "    tfds = None\n",
        "\n",
        "# Helper to install a specific version of a package\n",
        "def install_package(package, version):\n",
        "    subprocess.check_call([\"pip\", \"install\", f\"{package}=={version}\"])\n",
        "\n",
        "# Helper to uninstall multiple packages\n",
        "def uninstall_packages(packages):\n",
        "    subprocess.check_call([\"pip\", \"uninstall\", \"-y\"] + packages)\n",
        "\n",
        "# Version mismatch check\n",
        "version_mismatch = (\n",
        "    tf.__version__ != tf_version or\n",
        "    (hub and hub.__version__ != hub_version) or\n",
        "    (tfds and tfds.__version__ != datasets_version) or\n",
        "    not np.__version__.startswith(numpy_version)\n",
        ")\n",
        "\n",
        "# Act on mismatches\n",
        "if version_mismatch:\n",
        "    print(f\"TensorFlow: {tf.__version__} → {tf_version}\")\n",
        "    if hub: print(f\"TensorFlow Hub: {hub.__version__} → {hub_version}\")\n",
        "    if tfds: print(f\"TensorFlow Datasets: {tfds.__version__} → {datasets_version}\")\n",
        "    print(f\"NumPy: {np.__version__} → {numpy_version}\")\n",
        "\n",
        "    uninstall_packages([\n",
        "        \"tensorflow\", \"tensorflow_hub\", \"tensorflow_datasets\",\n",
        "        \"numpy\", \"protobuf\"\n",
        "    ])\n",
        "\n",
        "    install_package(\"tensorflow\", tf_version)\n",
        "    install_package(\"tensorflow_hub\", hub_version)\n",
        "    install_package(\"tensorflow_datasets\", datasets_version)\n",
        "    install_package(\"numpy\", numpy_version)\n",
        "    install_package(\"protobuf\", protobuf_version)\n",
        "\n",
        "    print(\"\\nRequired versions installed successfully.\")\n",
        "    print(\"Please restart the runtime (Runtime > Restart session) and re-run the notebook.\")\n",
        "else:\n",
        "    print(\"All packages are already at the specified versions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNRQoc7W17k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I9rzI4i1IHu"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0razZ_p07Tc"
      },
      "outputs": [],
      "source": [
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (224, 224)) / 255.0\n",
        "    return  image, label\n",
        "\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'rock_paper_scissors', split=['train[:80%]', 'train[80%:]', 'test'],\n",
        "    with_info=True, as_supervised=True)\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q96GrWjl1RfF"
      },
      "source": [
        "### Build and train a baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inIK227eZbgV"
      },
      "outputs": [],
      "source": [
        "module_selection = (\"mobilenet_v2\", 224, 1280)\n",
        "handle_base, pixels, FV_SIZE = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and output dimension {}\".format(MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))\n",
        "\n",
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,),\n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=False)\n",
        "\n",
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "hist = model.fit(train_batches,\n",
        "                 epochs=EPOCHS,\n",
        "                 validation_data=validation_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsWYz9e11dJc"
      },
      "source": [
        "### Save the baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cML-V4yqtuYI"
      },
      "outputs": [],
      "source": [
        "ROCK_PAPER_SCISSORS_SAVED_MODEL = \"exp_saved_model\"\n",
        "tf.saved_model.save(model, ROCK_PAPER_SCISSORS_SAVED_MODEL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMXNhkA41ftZ"
      },
      "source": [
        "## Now its your turn\n",
        "### Convert the model to TFLite\n",
        "\n",
        "Hint: You'll need to use the saved model to generate a converter and then run it. This will only require two lines of code! Isn't TFLite amazing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gButZXqZt3o4"
      },
      "outputs": [],
      "source": [
        "converter = # YOUR CODE GOES HERE #\n",
        "tflite_model = # YOUR CODE GOES HERE #\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/\")\n",
        "tflite_model_file = tflite_models_dir/'model1.tflite'\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "# This will report back the file size in bytes\n",
        "# you will note that this model is too big for our Arduino\n",
        "# but would work on a mobile phone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTmOU_9v2yWC"
      },
      "source": [
        "### Test the model\n",
        "\n",
        "Hint: You'll need to definite an Interpreter that we can use to run the model. Again TFLite being amazing will only require you to write one line of code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsNjuPhxuDOx"
      },
      "outputs": [],
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "TFLITE_MODEL_FILE = '/tmp/model1.tflite'\n",
        "interpreter = # YOUR CODE GOES HERE #\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT9x2iWv3dwS"
      },
      "outputs": [],
      "source": [
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "predictions = []\n",
        "\n",
        "# This will report how many iterations per second, where each\n",
        "# iteration is 100 predictions\n",
        "test_labels, test_imgs = [], []\n",
        "for img, label in tqdm(test_batches.take(100)):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    test_labels.append(label.numpy()[0])\n",
        "    test_imgs.append(img)\n",
        "\n",
        "# This will tell you how many of the predictions were correct\n",
        "score = 0\n",
        "for item in range(0,99):\n",
        "  prediction=np.argmax(predictions[item])\n",
        "  label = test_labels[item]\n",
        "  if prediction==label:\n",
        "    score=score+1\n",
        "\n",
        "print(\"\\nOut of 100 predictions I got \" + str(score) + \" correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJderXfY3rsE"
      },
      "source": [
        "## Plot the results\n",
        "\n",
        "To explore the results a little more we've include the plotting code below which will help you visualize which images the model is getting correct and incorrect. **If you'd like to improve the model's performance we suggest modifying the code in \"Build and train a baseline model.\" Some ideas include, training for more epochs, choosing a different model architecture, and changing the optimizer. If you are feeling ambitious we'd suggest trying out quantization aware training!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgWsPmDuJEI"
      },
      "outputs": [],
      "source": [
        "# Utilities for plotting\n",
        "\n",
        "class_names = ['rock', 'paper', 'scissors']\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    img = np.squeeze(img)\n",
        "\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         class_names[true_label]), color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TylXFi0quLEw"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the outputs { run: \"auto\" }\n",
        "max_index = 73 #@param {type:\"slider\", min:0, max:99, step:1}\n",
        "for index in range(0,max_index):\n",
        "  plt.figure(figsize=(6,3))\n",
        "  plt.subplot(1,2,1)\n",
        "  plot_image(index, predictions, test_labels, test_imgs)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRMm2v9t6bbj"
      },
      "source": [
        "#Further Study\n",
        "\n",
        "We are going to dive a little deeper into the ideas behind post-training quantization and optimization in the next section. That said, if you'd like to check out how these steps are implemetned in Tensorflow Lite, please check out the user guides at https://www.tensorflow.org/lite/performance/post_training_quantization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3-3-14-Assignment.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}